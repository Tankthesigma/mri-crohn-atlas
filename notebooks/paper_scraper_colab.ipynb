{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“š ISEF Paper Scraper - Perianal Crohn's Disease\n",
        "\n",
        "**Purpose:** Download research papers from PubMed Central for systematic review.\n",
        "\n",
        "**Strategy:** Broad search queries + strict local relevance filter.\n",
        "\n",
        "**Output:** PDFs saved to Google Drive in cluster subfolders.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 1: Setup & Dependencies"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "# Install BioPython for Entrez API access\n",
        "!pip install biopython -q\n",
        "\n",
        "# Mount Google Drive for persistent storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"âœ… Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 2: Configuration"
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION - EDIT THESE IF NEEDED\n",
        "# ============================================================================\n",
        "\n",
        "# Google Drive save location\n",
        "SAVE_DIR = Path(\"/content/drive/MyDrive/ISEF_Papers\")\n",
        "\n",
        "# Entrez API credentials\n",
        "ENTREZ_EMAIL = \"johs03047@gmail.com\"\n",
        "ENTREZ_API_KEY = \"e6dbad0565fd2136d119de63f233dbd92108\"\n",
        "\n",
        "# Search settings\n",
        "RETMAX = 500  # Papers per search term\n",
        "RATE_LIMIT = 10  # Requests per second (max with API key)\n",
        "\n",
        "# Create directory structure\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "(SAVE_DIR / \"Biologics\").mkdir(exist_ok=True)\n",
        "(SAVE_DIR / \"Surgical\").mkdir(exist_ok=True)\n",
        "(SAVE_DIR / \"Regenerative\").mkdir(exist_ok=True)\n",
        "(SAVE_DIR / \"General\").mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"ðŸ“ Save directory: {SAVE_DIR}\")\n",
        "print(f\"ðŸ“§ Email: {ENTREZ_EMAIL}\")\n",
        "print(f\"ðŸ”‘ API Key: {ENTREZ_API_KEY[:10]}...\")\n",
        "print(f\"ðŸ“Š RetMax: {RETMAX} papers/term\")\n",
        "print(\"âœ… Configuration complete!\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 3: Paper Scraper Engine"
      ],
      "metadata": {
        "id": "engine_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import time\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Tuple, Set\n",
        "from dataclasses import dataclass, field, asdict\n",
        "import threading\n",
        "import requests\n",
        "from Bio import Entrez\n",
        "\n",
        "# ============================================================================\n",
        "# SETUP ENTREZ\n",
        "# ============================================================================\n",
        "\n",
        "Entrez.email = ENTREZ_EMAIL\n",
        "Entrez.api_key = ENTREZ_API_KEY\n",
        "\n",
        "REQUEST_INTERVAL = 1.0 / RATE_LIMIT\n",
        "_last_request_time = 0\n",
        "_rate_lock = threading.Lock()\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s | %(levelname)s | %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ============================================================================\n",
        "# SEARCH TERMS WITH CLUSTER ASSIGNMENT\n",
        "# ============================================================================\n",
        "\n",
        "SEARCH_TERMS = [\n",
        "    # Biologics cluster\n",
        "    {'query': '(\"Infliximab\" AND \"Fistula\")', 'cluster': 'Biologics'},\n",
        "    {'query': '(\"Adalimumab\" AND \"Fistula\")', 'cluster': 'Biologics'},\n",
        "    {'query': '(\"Ustekinumab\" AND \"Fistula\")', 'cluster': 'Biologics'},\n",
        "    {'query': '(\"Vedolizumab\" AND \"Fistula\")', 'cluster': 'Biologics'},\n",
        "\n",
        "    # Surgical cluster\n",
        "    {'query': '(\"Seton\" AND \"Fistula\")', 'cluster': 'Surgical'},\n",
        "    {'query': '(\"Fistulotomy\" AND \"Fistula\")', 'cluster': 'Surgical'},\n",
        "    {'query': '(\"LIFT\" AND \"Fistula\")', 'cluster': 'Surgical'},\n",
        "    {'query': '(\"VAAFT\" AND \"Fistula\")', 'cluster': 'Surgical'},\n",
        "    {'query': '(\"Advancement Flap\" AND \"Fistula\")', 'cluster': 'Surgical'},\n",
        "\n",
        "    # Regenerative cluster\n",
        "    {'query': '(\"Stem Cells\" AND \"Fistula\")', 'cluster': 'Regenerative'},\n",
        "    {'query': '(\"Darvadstrocel\" AND \"Fistula\")', 'cluster': 'Regenerative'},\n",
        "    {'query': '(\"Fibrin Glue\" AND \"Fistula\")', 'cluster': 'Regenerative'},\n",
        "\n",
        "    # General/broad cluster\n",
        "    {'query': '(\"Perianal\" AND \"Crohn\")', 'cluster': 'General'},\n",
        "    {'query': '(\"Anal Fistula\" AND \"Treatment\")', 'cluster': 'General'},\n",
        "    {'query': '(\"Fistula-in-ano\" AND \"MRI\")', 'cluster': 'General'},\n",
        "]\n",
        "\n",
        "# ============================================================================\n",
        "# RELEVANCE FILTER\n",
        "# ============================================================================\n",
        "\n",
        "LOCATION_KEYWORDS = {\n",
        "    \"perianal\", \"peri-anal\", \"peri anal\",\n",
        "    \"anal\", \"rectovaginal\", \"anorectal\",\n",
        "    \"ano-rectal\", \"recto-vaginal\",\n",
        "    \"anoperineal\", \"ano-perineal\",\n",
        "    \"perineal\", \"cryptoglandular\",\n",
        "    \"fistula-in-ano\", \"fistula in ano\"\n",
        "}\n",
        "\n",
        "PATHOLOGY_KEYWORDS = {\n",
        "    \"fistula\", \"fistulae\", \"fistulizing\", \"fistulising\",\n",
        "    \"crohn\", \"crohn's\", \"crohns\",\n",
        "    \"ibd\", \"inflammatory bowel\",\n",
        "    \"abscess\", \"abscesses\",\n",
        "    \"pfcd\"\n",
        "}\n",
        "\n",
        "\n",
        "def is_relevant(text: str) -> Tuple[bool, Dict[str, List[str]]]:\n",
        "    \"\"\"Returns True if text contains BOTH location AND pathology keywords.\"\"\"\n",
        "    if not text:\n",
        "        return False, {\"location\": [], \"pathology\": []}\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    location_matches = [kw for kw in LOCATION_KEYWORDS if kw in text_lower]\n",
        "    pathology_matches = [kw for kw in PATHOLOGY_KEYWORDS if kw in text_lower]\n",
        "\n",
        "    is_rel = len(location_matches) > 0 and len(pathology_matches) > 0\n",
        "    return is_rel, {\"location\": location_matches, \"pathology\": pathology_matches}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DATA CLASSES\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class PRISMALog:\n",
        "    start_time: str = \"\"\n",
        "    end_time: str = \"\"\n",
        "    search_terms: Dict[str, Dict] = field(default_factory=dict)\n",
        "    totals: Dict[str, int] = field(default_factory=lambda: {\n",
        "        \"total_found\": 0,\n",
        "        \"unique_papers\": 0,\n",
        "        \"relevant\": 0,\n",
        "        \"downloaded\": 0,\n",
        "        \"skipped_irrelevant\": 0,\n",
        "        \"skipped_no_pdf\": 0,\n",
        "        \"skipped_error\": 0,\n",
        "        \"duplicates_removed\": 0\n",
        "    })\n",
        "    errors: List[Dict] = field(default_factory=list)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# RATE LIMITER\n",
        "# ============================================================================\n",
        "\n",
        "def rate_limited_request():\n",
        "    \"\"\"Ensures we don't exceed rate limit.\"\"\"\n",
        "    global _last_request_time\n",
        "    with _rate_lock:\n",
        "        now = time.time()\n",
        "        elapsed = now - _last_request_time\n",
        "        if elapsed < REQUEST_INTERVAL:\n",
        "            time.sleep(REQUEST_INTERVAL - elapsed)\n",
        "        _last_request_time = time.time()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PMC FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def search_pmc(query: str, retmax: int = RETMAX) -> Tuple[List[str], int]:\n",
        "    \"\"\"Search PMC directly for free full text papers.\"\"\"\n",
        "    rate_limited_request()\n",
        "\n",
        "    try:\n",
        "        handle = Entrez.esearch(\n",
        "            db=\"pmc\",\n",
        "            term=query,\n",
        "            retmax=retmax,\n",
        "            sort=\"relevance\"\n",
        "        )\n",
        "        record = Entrez.read(handle)\n",
        "        handle.close()\n",
        "\n",
        "        pmcids = record.get(\"IdList\", [])\n",
        "        total_count = int(record.get(\"Count\", 0))\n",
        "\n",
        "        logger.info(f\"Query: {query[:50]}... -> {total_count} total, {len(pmcids)} returned\")\n",
        "        return pmcids, total_count\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Search error: {e}\")\n",
        "        return [], 0\n",
        "\n",
        "\n",
        "def fetch_pmc_details(pmcids: List[str]) -> List[Dict]:\n",
        "    \"\"\"Fetch metadata for PMC articles.\"\"\"\n",
        "    if not pmcids:\n",
        "        return []\n",
        "\n",
        "    all_papers = []\n",
        "    batch_size = 50\n",
        "\n",
        "    for i in range(0, len(pmcids), batch_size):\n",
        "        batch = pmcids[i:i + batch_size]\n",
        "        rate_limited_request()\n",
        "\n",
        "        try:\n",
        "            handle = Entrez.efetch(\n",
        "                db=\"pmc\",\n",
        "                id=\",\".join(batch),\n",
        "                rettype=\"xml\",\n",
        "                retmode=\"xml\"\n",
        "            )\n",
        "            content = handle.read()\n",
        "            handle.close()\n",
        "\n",
        "            papers = parse_pmc_xml(content, batch)\n",
        "            all_papers.extend(papers)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Fetch error for batch {i//batch_size + 1}: {e}\")\n",
        "            # Fallback to individual fetches\n",
        "            for pmcid in batch:\n",
        "                paper = fetch_single_pmc(pmcid)\n",
        "                if paper:\n",
        "                    all_papers.append(paper)\n",
        "\n",
        "    return all_papers\n",
        "\n",
        "\n",
        "def fetch_single_pmc(pmcid: str) -> Optional[Dict]:\n",
        "    \"\"\"Fetch single PMC article metadata.\"\"\"\n",
        "    rate_limited_request()\n",
        "\n",
        "    try:\n",
        "        handle = Entrez.esummary(db=\"pmc\", id=pmcid)\n",
        "        record = Entrez.read(handle)\n",
        "        handle.close()\n",
        "\n",
        "        if record:\n",
        "            doc = record[0]\n",
        "            return {\n",
        "                \"pmcid\": f\"PMC{pmcid}\",\n",
        "                \"title\": doc.get(\"Title\", \"\"),\n",
        "                \"authors\": doc.get(\"AuthorList\", []),\n",
        "                \"first_author\": doc.get(\"AuthorList\", [\"Unknown\"])[0].split()[0] if doc.get(\"AuthorList\") else \"Unknown\",\n",
        "                \"year\": str(doc.get(\"PubDate\", \"\"))[:4],\n",
        "                \"journal\": doc.get(\"Source\", \"\"),\n",
        "                \"abstract\": \"\",\n",
        "                \"doi\": doc.get(\"DOI\", \"\")\n",
        "            }\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def parse_pmc_xml(xml_content: bytes, pmcids: List[str]) -> List[Dict]:\n",
        "    \"\"\"Parse PMC XML response.\"\"\"\n",
        "    papers = []\n",
        "\n",
        "    try:\n",
        "        content = xml_content.decode('utf-8', errors='ignore')\n",
        "        articles = re.split(r'<article[^>]*>', content)\n",
        "\n",
        "        for i, article in enumerate(articles[1:], 1):\n",
        "            try:\n",
        "                paper = {}\n",
        "\n",
        "                # PMC ID\n",
        "                pmcid_match = re.search(r'<article-id pub-id-type=\"pmc\">(\\d+)</article-id>', article)\n",
        "                paper[\"pmcid\"] = f\"PMC{pmcid_match.group(1)}\" if pmcid_match else f\"PMC{pmcids[i-1] if i-1 < len(pmcids) else ''}\"\n",
        "\n",
        "                # Title\n",
        "                title_match = re.search(r'<article-title[^>]*>(.+?)</article-title>', article, re.DOTALL)\n",
        "                paper[\"title\"] = re.sub(r'<[^>]+>', '', title_match.group(1)).strip() if title_match else \"\"\n",
        "\n",
        "                # Authors\n",
        "                authors = []\n",
        "                author_matches = re.findall(r'<surname>([^<]+)</surname>\\s*<given-names>([^<]+)</given-names>', article)\n",
        "                for surname, given in author_matches:\n",
        "                    authors.append(f\"{surname} {given}\")\n",
        "                paper[\"authors\"] = authors\n",
        "                paper[\"first_author\"] = authors[0].split()[0] if authors else \"Unknown\"\n",
        "\n",
        "                # Year\n",
        "                year_match = re.search(r'<year>(\\d{4})</year>', article)\n",
        "                paper[\"year\"] = year_match.group(1) if year_match else \"Unknown\"\n",
        "\n",
        "                # Journal\n",
        "                journal_match = re.search(r'<journal-title[^>]*>([^<]+)</journal-title>', article)\n",
        "                paper[\"journal\"] = journal_match.group(1).strip() if journal_match else \"\"\n",
        "\n",
        "                # Abstract\n",
        "                abstract_match = re.search(r'<abstract[^>]*>(.+?)</abstract>', article, re.DOTALL)\n",
        "                if abstract_match:\n",
        "                    paper[\"abstract\"] = re.sub(r'<[^>]+>', ' ', abstract_match.group(1)).strip()\n",
        "                    paper[\"abstract\"] = re.sub(r'\\s+', ' ', paper[\"abstract\"])\n",
        "                else:\n",
        "                    paper[\"abstract\"] = \"\"\n",
        "\n",
        "                # DOI\n",
        "                doi_match = re.search(r'<article-id pub-id-type=\"doi\">([^<]+)</article-id>', article)\n",
        "                paper[\"doi\"] = doi_match.group(1) if doi_match else \"\"\n",
        "\n",
        "                # Keywords\n",
        "                keywords = re.findall(r'<kwd>([^<]+)</kwd>', article)\n",
        "                paper[\"keywords\"] = keywords\n",
        "\n",
        "                if paper[\"pmcid\"] and (paper[\"title\"] or paper[\"abstract\"]):\n",
        "                    papers.append(paper)\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"XML parse error: {e}\")\n",
        "\n",
        "    return papers\n",
        "\n",
        "\n",
        "def get_pmc_pdf_url(pmcid: str) -> Optional[str]:\n",
        "    \"\"\"Get PDF download URL from PMC.\"\"\"\n",
        "    pmc_num = pmcid.replace(\"PMC\", \"\")\n",
        "    rate_limited_request()\n",
        "\n",
        "    try:\n",
        "        url = f\"https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id=PMC{pmc_num}\"\n",
        "        response = requests.get(url, timeout=30)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            content = response.text\n",
        "\n",
        "            pdf_match = re.search(r'href=\"([^\"]+\\.pdf)\"', content)\n",
        "            if pdf_match:\n",
        "                pdf_url = pdf_match.group(1)\n",
        "                if pdf_url.startswith(\"ftp://\"):\n",
        "                    pdf_url = pdf_url.replace(\"ftp://ftp.ncbi.nlm.nih.gov\", \"https://ftp.ncbi.nlm.nih.gov\")\n",
        "                return pdf_url\n",
        "\n",
        "        return f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmc_num}/pdf/\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmc_num}/pdf/\"\n",
        "\n",
        "\n",
        "def download_pdf(url: str, save_path: Path) -> Tuple[bool, Optional[str]]:\n",
        "    \"\"\"Download PDF from URL.\"\"\"\n",
        "    rate_limited_request()\n",
        "\n",
        "    try:\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\"\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, headers=headers, timeout=60, stream=True, allow_redirects=True)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
        "\n",
        "            if \"pdf\" in content_type or url.endswith(\".pdf\") or response.content[:4] == b'%PDF':\n",
        "                save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                with open(save_path, \"wb\") as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "\n",
        "                if save_path.stat().st_size > 5000:\n",
        "                    return True, None\n",
        "                else:\n",
        "                    save_path.unlink(missing_ok=True)\n",
        "                    return False, \"File too small\"\n",
        "            else:\n",
        "                return False, f\"Not PDF: {content_type[:30]}\"\n",
        "        else:\n",
        "            return False, f\"HTTP {response.status_code}\"\n",
        "\n",
        "    except requests.Timeout:\n",
        "        return False, \"Timeout\"\n",
        "    except Exception as e:\n",
        "        return False, str(e)[:50]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PROGRESS SAVING\n",
        "# ============================================================================\n",
        "\n",
        "PROGRESS_FILE = SAVE_DIR / \"download_progress.json\"\n",
        "METADATA_FILE = SAVE_DIR / \"paper_metadata.json\"\n",
        "LOG_FILE = SAVE_DIR / \"download_log.json\"\n",
        "\n",
        "\n",
        "def save_progress(prisma_log: PRISMALog, all_metadata: List[Dict], seen_pmcids: Set[str]):\n",
        "    \"\"\"Save progress for resumability.\"\"\"\n",
        "    progress = {\n",
        "        \"prisma_log\": asdict(prisma_log),\n",
        "        \"seen_pmcids\": list(seen_pmcids),\n",
        "        \"paper_count\": len(all_metadata)\n",
        "    }\n",
        "    with open(PROGRESS_FILE, \"w\") as f:\n",
        "        json.dump(progress, f, indent=2)\n",
        "\n",
        "    with open(METADATA_FILE, \"w\") as f:\n",
        "        json.dump(all_metadata, f, indent=2)\n",
        "\n",
        "\n",
        "def load_progress() -> Tuple[Optional[PRISMALog], Set[str], List[Dict]]:\n",
        "    \"\"\"Load previous progress if exists.\"\"\"\n",
        "    if not PROGRESS_FILE.exists():\n",
        "        return None, set(), []\n",
        "\n",
        "    try:\n",
        "        with open(PROGRESS_FILE, \"r\") as f:\n",
        "            progress = json.load(f)\n",
        "\n",
        "        log_data = progress.get(\"prisma_log\", {})\n",
        "        prisma_log = PRISMALog(\n",
        "            start_time=log_data.get(\"start_time\", \"\"),\n",
        "            end_time=log_data.get(\"end_time\", \"\"),\n",
        "            search_terms=log_data.get(\"search_terms\", {}),\n",
        "            totals=log_data.get(\"totals\", PRISMALog().totals),\n",
        "            errors=log_data.get(\"errors\", [])\n",
        "        )\n",
        "\n",
        "        seen_pmcids = set(progress.get(\"seen_pmcids\", []))\n",
        "\n",
        "        metadata = []\n",
        "        if METADATA_FILE.exists():\n",
        "            with open(METADATA_FILE, \"r\") as f:\n",
        "                metadata = json.load(f)\n",
        "\n",
        "        logger.info(f\"âœ… Resumed from progress: {len(seen_pmcids)} papers seen\")\n",
        "        return prisma_log, seen_pmcids, metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not load progress: {e}\")\n",
        "        return None, set(), []\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def sanitize_filename(name: str) -> str:\n",
        "    \"\"\"Create safe filename.\"\"\"\n",
        "    name = re.sub(r'[<>:\"/\\\\|?*\\'\\[\\]()]', '', name)\n",
        "    name = re.sub(r'\\s+', '_', name)\n",
        "    return name[:40]\n",
        "\n",
        "\n",
        "def process_search_term(term_info: Dict, prisma_log: PRISMALog, seen_pmcids: Set[str],\n",
        "                        all_metadata: List[Dict]) -> int:\n",
        "    \"\"\"Process a single search term.\"\"\"\n",
        "\n",
        "    query = term_info['query']\n",
        "    cluster = term_info['cluster']\n",
        "    term_name = query.replace('\"', '').replace('(', '').replace(')', '')[:40]\n",
        "\n",
        "    if term_name in prisma_log.search_terms:\n",
        "        logger.info(f\"â­ï¸ Skipping already processed: {term_name}\")\n",
        "        return 0\n",
        "\n",
        "    term_log = {\n",
        "        \"query\": query,\n",
        "        \"cluster\": cluster,\n",
        "        \"found\": 0,\n",
        "        \"new\": 0,\n",
        "        \"relevant\": 0,\n",
        "        \"downloaded\": 0,\n",
        "        \"skipped_irrelevant\": 0,\n",
        "        \"skipped_duplicate\": 0,\n",
        "        \"skipped_error\": 0\n",
        "    }\n",
        "\n",
        "    # Search PMC\n",
        "    pmcids, total_count = search_pmc(query)\n",
        "    term_log[\"found\"] = total_count\n",
        "    prisma_log.totals[\"total_found\"] += len(pmcids)\n",
        "\n",
        "    if not pmcids:\n",
        "        prisma_log.search_terms[term_name] = term_log\n",
        "        return 0\n",
        "\n",
        "    # Remove duplicates\n",
        "    new_pmcids = [p for p in pmcids if f\"PMC{p}\" not in seen_pmcids]\n",
        "    term_log[\"skipped_duplicate\"] = len(pmcids) - len(new_pmcids)\n",
        "    term_log[\"new\"] = len(new_pmcids)\n",
        "    prisma_log.totals[\"duplicates_removed\"] += term_log[\"skipped_duplicate\"]\n",
        "\n",
        "    logger.info(f\"  ðŸ“„ New papers: {len(new_pmcids)} (skipped {term_log['skipped_duplicate']} duplicates)\")\n",
        "\n",
        "    if not new_pmcids:\n",
        "        prisma_log.search_terms[term_name] = term_log\n",
        "        return 0\n",
        "\n",
        "    # Fetch metadata\n",
        "    papers = fetch_pmc_details(new_pmcids)\n",
        "\n",
        "    downloaded = 0\n",
        "    cluster_dir = SAVE_DIR / cluster\n",
        "\n",
        "    for paper in papers:\n",
        "        pmcid = paper.get(\"pmcid\", \"\")\n",
        "        if not pmcid:\n",
        "            continue\n",
        "\n",
        "        seen_pmcids.add(pmcid)\n",
        "        prisma_log.totals[\"unique_papers\"] += 1\n",
        "\n",
        "        # Combine text for relevance check\n",
        "        text_to_check = \" \".join([\n",
        "            paper.get(\"title\", \"\"),\n",
        "            paper.get(\"abstract\", \"\"),\n",
        "            \" \".join(paper.get(\"keywords\", []))\n",
        "        ])\n",
        "\n",
        "        # RELEVANCE FILTER\n",
        "        is_rel, matches = is_relevant(text_to_check)\n",
        "\n",
        "        if not is_rel:\n",
        "            term_log[\"skipped_irrelevant\"] += 1\n",
        "            prisma_log.totals[\"skipped_irrelevant\"] += 1\n",
        "            continue\n",
        "\n",
        "        term_log[\"relevant\"] += 1\n",
        "        prisma_log.totals[\"relevant\"] += 1\n",
        "\n",
        "        # Generate filename\n",
        "        safe_author = sanitize_filename(paper.get(\"first_author\", \"Unknown\"))\n",
        "        year = paper.get(\"year\", \"Unknown\")\n",
        "        safe_query = sanitize_filename(term_name.split(\"AND\")[0])\n",
        "        filename = f\"{safe_author}_{year}_{safe_query}_{pmcid}.pdf\"\n",
        "        pdf_path = cluster_dir / filename\n",
        "\n",
        "        # Skip if already downloaded\n",
        "        if pdf_path.exists() and pdf_path.stat().st_size > 5000:\n",
        "            paper[\"pdf_path\"] = str(pdf_path)\n",
        "            paper[\"status\"] = \"exists\"\n",
        "            paper[\"cluster\"] = cluster\n",
        "            paper[\"relevance_matches\"] = matches\n",
        "            paper[\"search_term\"] = query\n",
        "            all_metadata.append(paper)\n",
        "            term_log[\"downloaded\"] += 1\n",
        "            downloaded += 1\n",
        "            continue\n",
        "\n",
        "        # Get PDF URL and download\n",
        "        pdf_url = get_pmc_pdf_url(pmcid)\n",
        "\n",
        "        if pdf_url:\n",
        "            success, error = download_pdf(pdf_url, pdf_path)\n",
        "\n",
        "            if success:\n",
        "                paper[\"pdf_path\"] = str(pdf_path)\n",
        "                paper[\"status\"] = \"downloaded\"\n",
        "                paper[\"cluster\"] = cluster\n",
        "                paper[\"relevance_matches\"] = matches\n",
        "                paper[\"search_term\"] = query\n",
        "                all_metadata.append(paper)\n",
        "                term_log[\"downloaded\"] += 1\n",
        "                prisma_log.totals[\"downloaded\"] += 1\n",
        "                downloaded += 1\n",
        "                logger.info(f\"    âœ… {filename[:50]}\")\n",
        "            else:\n",
        "                paper[\"status\"] = \"error\"\n",
        "                paper[\"error\"] = error\n",
        "                paper[\"cluster\"] = cluster\n",
        "                paper[\"search_term\"] = query\n",
        "                all_metadata.append(paper)\n",
        "                term_log[\"skipped_error\"] += 1\n",
        "                prisma_log.totals[\"skipped_error\"] += 1\n",
        "        else:\n",
        "            term_log[\"skipped_error\"] += 1\n",
        "            prisma_log.totals[\"skipped_no_pdf\"] += 1\n",
        "\n",
        "    prisma_log.search_terms[term_name] = term_log\n",
        "\n",
        "    # Save progress after each term\n",
        "    save_progress(prisma_log, all_metadata, seen_pmcids)\n",
        "\n",
        "    return downloaded\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution.\"\"\"\n",
        "    print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘       ðŸ“š ISEF PAPER SCRAPER - PERIANAL CROHN'S DISEASE ðŸ“š            â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘  Strategy: Broad PMC search + strict relevance filter               â•‘\n",
        "â•‘  Search Terms: 15 queries across 4 clusters                         â•‘\n",
        "â•‘  RetMax: 500 per term                                               â•‘\n",
        "â•‘  Output: Google Drive (cluster subfolders)                          â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    \"\"\")\n",
        "\n",
        "    # Try to resume from progress\n",
        "    prisma_log, seen_pmcids, all_metadata = load_progress()\n",
        "\n",
        "    if prisma_log is None:\n",
        "        prisma_log = PRISMALog(start_time=datetime.now().isoformat())\n",
        "        seen_pmcids = set()\n",
        "        all_metadata = []\n",
        "        print(\"ðŸ†• Starting fresh run...\")\n",
        "    else:\n",
        "        print(f\"ðŸ”„ Resuming from {len(seen_pmcids)} previously seen papers...\")\n",
        "\n",
        "    total_downloaded = 0\n",
        "\n",
        "    # Process each search term\n",
        "    for i, term_info in enumerate(SEARCH_TERMS, 1):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"[{i}/{len(SEARCH_TERMS)}] {term_info['query']} -> {term_info['cluster']}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        try:\n",
        "            downloaded = process_search_term(term_info, prisma_log, seen_pmcids, all_metadata)\n",
        "            total_downloaded += downloaded\n",
        "            print(f\"  ðŸ“¥ Downloaded {downloaded} papers from this query\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing {term_info['query']}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Finalize\n",
        "    prisma_log.end_time = datetime.now().isoformat()\n",
        "\n",
        "    # Save final log\n",
        "    with open(LOG_FILE, \"w\") as f:\n",
        "        json.dump(asdict(prisma_log), f, indent=2)\n",
        "\n",
        "    # Save final metadata\n",
        "    with open(METADATA_FILE, \"w\") as f:\n",
        "        json.dump(all_metadata, f, indent=2)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸ“Š FINAL SUMMARY - PRISMA FLOW\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\n{'Metric':<45} {'Count':>10}\")\n",
        "    print(\"-\"*57)\n",
        "    print(f\"{'Total records found in PMC':<45} {prisma_log.totals['total_found']:>10}\")\n",
        "    print(f\"{'Unique papers (after dedup)':<45} {prisma_log.totals['unique_papers']:>10}\")\n",
        "    print(f\"{'Passed relevance filter':<45} {prisma_log.totals['relevant']:>10}\")\n",
        "    print(f\"{'Successfully downloaded':<45} {prisma_log.totals['downloaded']:>10}\")\n",
        "    print(\"-\"*57)\n",
        "    print(f\"{'Skipped - irrelevant':<45} {prisma_log.totals['skipped_irrelevant']:>10}\")\n",
        "    print(f\"{'Skipped - duplicates':<45} {prisma_log.totals['duplicates_removed']:>10}\")\n",
        "    print(f\"{'Skipped - download error':<45} {prisma_log.totals['skipped_error']:>10}\")\n",
        "    print(\"-\"*57)\n",
        "\n",
        "    print(\"\\n\\nðŸ“ BY CLUSTER:\")\n",
        "    print(\"-\"*70)\n",
        "    cluster_counts = {}\n",
        "    for term_name, term_data in prisma_log.search_terms.items():\n",
        "        cluster = term_data.get(\"cluster\", \"Unknown\")\n",
        "        if cluster not in cluster_counts:\n",
        "            cluster_counts[cluster] = 0\n",
        "        cluster_counts[cluster] += term_data.get(\"downloaded\", 0)\n",
        "\n",
        "    for cluster, count in cluster_counts.items():\n",
        "        print(f\"  {cluster}: {count} PDFs\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"ðŸ“ Output directory: {SAVE_DIR}\")\n",
        "    print(f\"ðŸ“‹ PRISMA log: {LOG_FILE}\")\n",
        "    print(f\"ðŸ“‹ Metadata: {METADATA_FILE}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    print(\"âœ… COMPLETE!\")\n",
        "\n",
        "    return prisma_log\n",
        "\n",
        "\n",
        "print(\"âœ… Engine loaded! Run main() in the next cell.\")"
      ],
      "metadata": {
        "id": "engine"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4: Run the Scraper"
      ],
      "metadata": {
        "id": "run_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the paper scraper!\n",
        "# This will:\n",
        "# 1. Search PMC for 15 broad queries\n",
        "# 2. Filter papers by relevance (must mention anal/perianal + fistula/Crohn)\n",
        "# 3. Download PDFs to Google Drive in cluster folders\n",
        "# 4. Save progress after each term (resumable if interrupted)\n",
        "\n",
        "prisma_log = main()"
      ],
      "metadata": {
        "id": "run"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 5: Check Results (Optional)"
      ],
      "metadata": {
        "id": "check_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what was downloaded\n",
        "import os\n",
        "\n",
        "print(\"ðŸ“ Downloaded PDFs by cluster:\\n\")\n",
        "\n",
        "for cluster in [\"Biologics\", \"Surgical\", \"Regenerative\", \"General\"]:\n",
        "    cluster_path = SAVE_DIR / cluster\n",
        "    if cluster_path.exists():\n",
        "        pdfs = list(cluster_path.glob(\"*.pdf\"))\n",
        "        print(f\"{cluster}: {len(pdfs)} PDFs\")\n",
        "        for pdf in pdfs[:3]:  # Show first 3\n",
        "            print(f\"  - {pdf.name[:60]}...\")\n",
        "        if len(pdfs) > 3:\n",
        "            print(f\"  ... and {len(pdfs) - 3} more\")\n",
        "        print()\n",
        "\n",
        "# Total count\n",
        "total_pdfs = len(list(SAVE_DIR.glob(\"**/*.pdf\")))\n",
        "print(f\"\\nðŸ“Š TOTAL PDFs: {total_pdfs}\")"
      ],
      "metadata": {
        "id": "check"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
